---
title: "EC 349 Individual project"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2023-12-05"
---
## statement
We're part of an academic community at Warwick.
Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.
Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.
In submitting my work I confirm that:
1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.
2. I declare that the work is all my own, except where I have stated otherwise.
3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.
4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.
5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.
6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.
7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.
Privacy statement
The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Methodology

This assignment was approached through the John Rollins’ General DS Methodology. Starting off solving this problem by defining the phases needed: the problem definition phase, data organizing phase and finally the evaluating and deploying phase. Setting the problem to predict the stars within the ‘review_data’ this paper initially approached the problem by making assumptions on what independent variables should be the most correlated and help with the prediction. Accordingly, the data irrelevant to the assumption within in the given data set were eliminated data and the rest   reorganized or redefined and merged with data within the data set. followed naturally by the second phase of the method. This paper tries to incorporate as much of the understanding of the data through the assumption and the organization falls with it. Finally, evaluating whether or not the assumption stands and finding which might be more effective.

## Assumptions
This paper works on the assumption that for predicting review stars the responses to the review, former user behavior, business reputation, core business attributes including the price, facility, food, ambiance, image, location, and service. Which are the six factors that can have an impact on attractive pricing (Longart, 2015). In general, the paper assumes not only the naturally thought of business reputation itself and the former responses or its reactions have an impact it also puts in the factor of what makes it more attractive including the price and have an impact on the overall experience. It has also been taken account that the reviews rely on the audience (Parikh, A. A., Behnke, C., Almanza, B., Nelson, D., & Vorvoreanu, M, 2017). So this paper tried to put in attributes of a business and since it is review site based on former user and business reputations input it as well. Though did not put in text analysis because the exact relationship might be hard to find.


```{r}
#Pre-Processing Yelp Academic Data for the Assignment
library(jsonlite)

#Clear
cat("\014")  
rm(list=ls())

#Set Directory as appropriate
#setwd("C/project/script/document")

#Load Different Data
business_data <- stream_in(file("yelp_academic_dataset_business.json")) #note that stream_in reads the json lines (as the files are json lines, not json)
review_data  <- stream_in(file("yelp_academic_dataset_review.json")) #note that stream_in reads the json lines (as the files are json lines, not json)
checkin_data  <- stream_in(file("yelp_academic_dataset_checkin.json")) #note that stream_in reads the json lines (as the files are json lines, not json)
user_data <- stream_in(file("yelp_academic_dataset_user.json")) #note that stream_in reads the json lines (as the files are json lines, not json)
tip_data  <- stream_in(file("yelp_academic_dataset_tip.json")) #note that stream_in reads the json lines (as the files are json lines, not json)
```
## Data organizing
The number of stars can be thought out through the response from the reviews which are represented as ‘useful’, ‘cool’, ‘funny’, past user behavior represented by ‘average_stars’ because it was thought to most decisive compared to how the user reacted to other’s comments and how many people respond to them, received business reviews represented by how much stars and reviews the business has got so far and review count being modified in to 5 quintiles to match the scoring system of the stars, lastly the business attributes which as explained in the assumption are divided into price, facility, food and quality, ambience , image, location , service related. Each of the business attributes were represented by variables explained in the code.Each data was selected to be the most representative of the chosen titles. After doing so the six other categories were then grouped into ‘totalevaluaion_attributes’. In each of the columns the ones that had the attribute were filled with 1 and the rest zero and the columns were merged so that it captured the averages of the merged columns. Finally, when being grouped into the final stage added up so that it had a business can score a total of 6 for the given attributes. The blanks in the original data were filled with zero, and rest of the columns were either erased or not used at all in this analysis.

```{r}
# Specify columns to remove from the nested 'attributes' data frame
columns_to_remove_nested <- c("BikeParking", "CoatCheck","DriveThru", "BusinessAcceptsBitcoin", "GoodForDancing", "AcceptsInsurance", "BYOB", "Corkage", "BYOBCorkage", "HairSpecializesIn", "Open24Hours", "Caters", "HappyHour", "Dietaryrestrictions", "Smoking", "Bestnights","OutdoorSeating", "Restaurantscounterservice", "HasTV", "DogsAllowed", "AgesAllowed")

# Remove specified columns from the nested data frame
business_data$attributes <- business_data$attributes[, !(names(business_data$attributes) %in% columns_to_remove_nested)]

# Removing columns by name
columns_to_remove <- c("name", "address", "city", "state", "postal_code", "latitude", "longitude", "is_open")
business_data <- business_data[, !(names(business_data) %in% columns_to_remove)]

# Removing columns by name
columns_to_remove <- c("name", "yelping_since", "useful", "funny", "cool", "friends", "compliment_hot", "compliment_more", "compliment_profile", "compliment_cute", "compliment_list", "compliment_note", "compliment_plain", "compliment_cool", "compliment_funny", "compliment_writer", "compliment_photos")
user_data <- user_data[, !(names(user_data) %in% columns_to_remove)]
```


```{r}
# blanks filling
business_data <- apply(business_data, 2, function(x) replace(x, is.na(x), 0))
```

```{r}
library(dplyr)
business_data <- as_tibble(business_data)

# Specifing columns to transform
columns_to_transform <- c("attributes.ByAppointmentOnly", "attributes.BusinessAcceptsCreditCards", "attributes.RestaurantsTakeOut", "attributes.RestaurantsDelivery", "attributes.WheelchairAccessible", "attributes.RestaurantsReservations", "attributes.GoodForKids", "attributes.RestaurantsTableService", "attributes.RestaurantsGoodForGroups")

# Apply the transformation to the specified columns
business_data <- business_data %>%
  mutate_at(vars(columns_to_transform), ~ifelse(. == "True", 1, 0))

# Removing columns by name
columns_to_remove <- c("attributes.Alcohol", "attributes.BestNights", "attributes.RestaurantsCounterService", "attributes.DietaryRestrictions", "attributes.WiFi")
business_data <- business_data[, !(names(business_data) %in% columns_to_remove)]
```


```{r}
# Data in business_data sorted(attributes)
library(dplyr)

business_data <- business_data %>%
  mutate(attributes.BusinessParking = ifelse(attributes.BusinessParking %in% c('None', 0), 0, 1))

business_data <- business_data %>%
  mutate(attributes.Ambience = ifelse(attributes.Ambience %in% c('None', 0), 0, 1))

business_data <- business_data %>%
  mutate(attributes.NoiseLevel = ifelse(attributes.NoiseLevel %in% c('quiet', 'average', "u'quiet'", "u'average'"), 1, 0))

business_data <- business_data %>%
  mutate(attributes.RestaurantsAttire = ifelse(attributes.RestaurantsAttire %in% c(0), 0, 1))

business_data <- business_data %>%
  mutate(attributes.GoodForMeal = ifelse(attributes.GoodForMeal %in% c(0), 0, 1))

business_data <- business_data %>%
  mutate(attributes.Music = ifelse(attributes.Music %in% c(0), 0, 1))
```


```{r}
library(dplyr)

business_data <- business_data %>%
  mutate(facilityrelated = rowMeans(cbind(attributes.BusinessParking, attributes.BusinessAcceptsCreditCards), na.rm = TRUE))

business_data <- business_data %>%
  mutate(food_qualityrelated = rowMeans(cbind(attributes.GoodForMeal, attributes.RestaurantsAttire), na.rm = TRUE))

business_data <- business_data %>%
  mutate(atmosphererelated = rowMeans(cbind(attributes.Ambience, attributes.NoiseLevel, attributes.Music), na.rm = TRUE))

business_data <- business_data %>%
  mutate(imagerelated = rowMeans(cbind(attributes.ByAppointmentOnly, attributes.RestaurantsReservations), na.rm = TRUE))

business_data <- business_data %>%
  mutate(servicerelated = rowMeans(cbind(attributes.RestaurantsTableService, attributes.GoodForKids, attributes.RestaurantsGoodForGroups), na.rm = TRUE))

business_data <- business_data %>%
  mutate(accesibilityrelated = rowMeans(cbind(attributes.WheelchairAccessible, attributes.RestaurantsDelivery, attributes.RestaurantsTakeOut), na.rm = TRUE))

business_data <- business_data %>%
  mutate(totalevaluation_attributes = facilityrelated + food_qualityrelated + atmosphererelated + imagerelated + servicerelated + accesibilityrelated)

business_data$attributes.RestaurantsPriceRange2 <- as.numeric(business_data$attributes.RestaurantsPriceRange2)
```

```{r}
# Convert 'review_count' to numeric
business_data$review_count <- as.numeric(business_data$review_count)

# Calculate quantiles
quantiles <- quantile(business_data$review_count, probs = seq(0, 1, 0.2), na.rm = TRUE)
business_data$quintile <- cut(business_data$review_count, breaks = quantiles, labels = FALSE, include.lowest = TRUE)
```
## The training and evaluation
Both the training data and test data were split from the ‘review_data’ and merged with the specific columns within the business and user data so that it fit the assumption. Each name was given ‘trainingdata_actual’ and ‘testdata_actual’. This paper uses two models the LASSO and the regression tree of the named as the decision tree within the code. The two methods the first one for the linear analysis and figuring out what variables are the most conclusive, the second being that given the nature of the predictive area a decision tree brings will be more effective. Each model was given four different data sets to compare with. The first one fitting all of the independent variables from the assumption the second one fitting parts from the review data, the third from the combination of business and user data the fourth fitting only business data. Each was given a specific meaning, the first the overall relevance, the second the comparison model, the third and the fourth each trying to prove the pure relevance of variables coming from the business data in particular.


```{r}
# setting training and test data
library(caret)
set.seed(1)

test_index <- createDataPartition(review_data$stars, p = 0.2, list = FALSE)
training_data <- review_data[-test_index, ]
test_data <- review_data[test_index, ]

colnames(user_data)
old_name <- "stars"
new_name <- "business_stars"

names(business_data)[names(business_data) == old_name] <- new_name
colnames(business_data)[colnames(business_data) == old_name] <- new_name

old_name <- "quintile"
new_name <- "quintile_reviewcount"

names(business_data)[names(business_data) == old_name] <- new_name
colnames(business_data)[colnames(business_data) == old_name] <- new_name
```

```{r}
# Training data change
# Select columns from business_data
selected_business_data <- business_data[c("business_id", "business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount")]

# Select columns from user_data
selected_user_data <- user_data[c("user_id", "average_stars")]

# Merge training_data, selected_business_data, and selected_user_data
trainingdata_actual <- merge(training_data, selected_business_data, by = "business_id", all.x = TRUE)
trainingdata_actual <- merge(trainingdata_actual, selected_user_data, by = "user_id", all.x = TRUE)

str(trainingdata_actual)
trainingdata_actual$business_stars <- as.numeric(trainingdata_actual$business_stars)

# test data change
# Select columns from business_data
selected_business_data <- business_data[c("business_id", "business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount")]

# Select columns from user_data
selected_user_data <- user_data[c("user_id", "average_stars")]

# Merge training_data, selected_business_data, and selected_user_data
testdata_actual <- merge(test_data, selected_business_data, by = "business_id", all.x = TRUE)
testdata_actual <- merge(testdata_actual, selected_user_data, by = "user_id", all.x = TRUE)

testdata_actual$business_stars <- as.numeric(testdata_actual$business_stars)

```
## LASSO
How the LASSO was run is in the explanation above. The mean squared error for each of the four was lowest from the first followed by the fourth, third and second. Looking deeper in it can be said that the average stars given from the users and the business stars already taken before were the most relevant and impact in prediction followed by one of the review remarks, price, total evaluation of the attributes respectively. It can be said overall the stars that were previously given had the most impact within this model.


```{r}
# Lasso_1 all the data
library(glmnet)

# names of the columns included in the LASSO model
selected_columns <- c("useful", "funny", "cool", "business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount", "average_stars")  

X_train_selected <- trainingdata_actual[, selected_columns]

X_train_selected_imputed <- apply(X_train_selected, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_train_imputed <- trainingdata_actual$stars
y_train_imputed <- ifelse(is.na(y_train_imputed), mean(y_train_imputed, na.rm = TRUE), y_train_imputed)

# Fit a LASSO model
lasso_model_selected <- cv.glmnet(x = as.matrix(X_train_selected_imputed), y = as.vector(y_train_imputed), alpha = 1, parallel = TRUE)

# Choose the lambda value
best_lambda_selected <- lasso_model_selected$lambda.min

# Fit the final LASSO model
final_lasso_model_selected <- glmnet(x = as.matrix(X_train_selected_imputed), y = as.vector(y_train_imputed), alpha = 1, lambda = best_lambda_selected, parallel = TRUE)

# Subset the test data to include only the selected columns
X_test_selected <- testdata_actual[, selected_columns]

X_test_selected_imputed <- apply(X_test_selected, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_test_imputed <- testdata_actual$stars
y_test_imputed <- ifelse(is.na(y_test_imputed), mean(y_test_imputed, na.rm = TRUE), y_test_imputed)

# Predict on the test data
lasso_predictions_selected <- predict(final_lasso_model_selected, newx = as.matrix(X_test_selected_imputed), s = best_lambda_selected, parallel = TRUE)

# Evaluate the model
mse_selected <- mean((lasso_predictions_selected - y_test_imputed)^2)

plot(lasso_predictions_selected)

# Get the coefficients
lasso_coefficients <- coef(final_lasso_model_selected, s = best_lambda_selected)
print(lasso_coefficients)

```


```{r}
#Lasso_2 review data based
library(glmnet)

# Including specified columns
selected_columns <- c("useful", "funny", "cool")  

# Subset the training data to include only the selected columns
X_train_selected_two <- trainingdata_actual[, selected_columns]

X_train_selected_imputed_two <- apply(X_train_selected_two, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_train_imputed_two <- trainingdata_actual$stars
y_train_imputed_two <- ifelse(is.na(y_train_imputed_two), mean(y_train_imputed_two, na.rm = TRUE), y_train_imputed_two)

# Fit a LASSO model
lasso_model_selected_two <- cv.glmnet(x = as.matrix(X_train_selected_imputed_two), y = as.vector(y_train_imputed_two), alpha = 1, parallel = TRUE)

# Choose the lambda value
best_lambda_selected_two <- lasso_model_selected_two$lambda.min

# Fit the final LASSO model
final_lasso_model_selected_two <- glmnet(x = as.matrix(X_train_selected_imputed_two), y = as.vector(y_train_imputed_two), alpha = 1, lambda = best_lambda_selected_two, parallel = TRUE)

# Subset the test data to include only the selected columns
X_test_selected_two <- testdata_actual[, selected_columns]

X_test_selected_imputed_two <- apply(X_test_selected_two, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_test_imputed_two <- testdata_actual$stars
y_test_imputed_two <- ifelse(is.na(y_test_imputed_two), mean(y_test_imputed_two, na.rm = TRUE), y_test_imputed_two)

# Predict on the test data 
lasso_predictions_selected_two <- predict(final_lasso_model_selected_two, newx = as.matrix(X_test_selected_imputed_two), s = best_lambda_selected_two, parallel = TRUE)

# Evaluate the model
mse_selected_two <- mean((lasso_predictions_selected_two - y_test_imputed_two)^2)

plot(lasso_predictions_selected_two)

lasso_coefficients_two <- coef(final_lasso_model_selected_two, s = best_lambda_selected_two)
print(lasso_coefficients_two)

```

```{r}
#Lasso 3 business and user data based
library(glmnet)

# Including specified columns
selected_columns <- c("business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount", "average_stars")  

# Subset the training data to include only the selected columns
X_train_selected_three <- trainingdata_actual[, selected_columns]

X_train_selected_imputed_three <- apply(X_train_selected_three, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_train_imputed_three <- trainingdata_actual$stars
y_train_imputed_three <- ifelse(is.na(y_train_imputed_three), mean(y_train_imputed_three, na.rm = TRUE), y_train_imputed_three)

# Fit a LASSO model
lasso_model_selected_three <- cv.glmnet(x = as.matrix(X_train_selected_imputed_three), y = as.vector(y_train_imputed_three), alpha = 1, parallel = TRUE)

# Choose the lambda value
best_lambda_selected_three <- lasso_model_selected_three$lambda.min

# Fit the final LASSO model
final_lasso_model_selected_three <- glmnet(x = as.matrix(X_train_selected_imputed_three), y = as.vector(y_train_imputed_three), alpha = 1, lambda = best_lambda_selected_three, parallel = TRUE)

# Subset the test data to include only the selected columns
X_test_selected_three <- testdata_actual[, selected_columns]

X_test_selected_imputed_three <- apply(X_test_selected_three, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_test_imputed_three <- testdata_actual$stars
y_test_imputed_three <- ifelse(is.na(y_test_imputed_three), mean(y_test_imputed_three, na.rm = TRUE), y_test_imputed_three)

# Predict on the test data 
lasso_predictions_selected_three <- predict(final_lasso_model_selected_three, newx = as.matrix(X_test_selected_imputed_three), s = best_lambda_selected_three, parallel = TRUE)

# Evaluate the model
mse_selected_three <- mean((lasso_predictions_selected_three - y_test_imputed_three)^2)

plot(lasso_predictions_selected_three)

# Get the coefficients for the final LASSO model
lasso_coefficients_three <- coef(final_lasso_model_selected_three, s = best_lambda_selected_three)
print(lasso_coefficients_three)

```

```{r}
#Lasso 4 business data based
library(glmnet)

# including specified columns
selected_columns <- c("business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount")  

# Subset the training data to include only the selected columns
X_train_selected_four <- trainingdata_actual[, selected_columns]

X_train_selected_imputed_four <- apply(X_train_selected_four, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_train_imputed_four <- trainingdata_actual$stars
y_train_imputed_four <- ifelse(is.na(y_train_imputed_four), mean(y_train_imputed_four, na.rm = TRUE), y_train_imputed_four)

# Fit a LASSO model
lasso_model_selected_four <- cv.glmnet(x = as.matrix(X_train_selected_imputed_four), y = as.vector(y_train_imputed_four), alpha = 1, parallel = TRUE)

# Choose the lambda value
best_lambda_selected_four <- lasso_model_selected_four$lambda.min

# Fit the final LASSO model
final_lasso_model_selected_four <- glmnet(x = as.matrix(X_train_selected_imputed_four), y = as.vector(y_train_imputed_four), alpha = 1, lambda = best_lambda_selected_four, parallel = TRUE)

# To include only the selected columns
X_test_selected_four <- testdata_actual[, selected_columns]

X_test_selected_imputed_four <- apply(X_test_selected_four, 2, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
y_test_imputed_four <- testdata_actual$stars
y_test_imputed_four <- ifelse(is.na(y_test_imputed_four), mean(y_test_imputed_four, na.rm = TRUE), y_test_imputed_four)

# Predict on the imputed test data
lasso_predictions_selected_four <- predict(final_lasso_model_selected_four, newx = as.matrix(X_test_selected_imputed_four), s = best_lambda_selected_four, parallel = TRUE)

# Evaluate the model
mse_selected_four <- mean((lasso_predictions_selected_four - y_test_imputed_four)^2)

plot(lasso_predictions_selected_four)

# Get the coefficients for the final LASSO model
lasso_coefficients_four <- coef(final_lasso_model_selected_four, s = best_lambda_selected_four)
print(lasso_coefficients_four)

```

## Decision tree
The decision trees were made and evaluated in the same way as the LASSO. The biggest part of the evaluation being business stars and the average stars given by the user. They were measured in the accuracies throughout the code the fourth being the highest in the rest irrelevant in comparison. The numbers of the accuracy might be failing it can be said that the actual predictions are not too far off mostly within the vicinity of the predictions of the decision tree(for example if the prediction is 3.8 stars of 4 being the most data from the actual test data). The failing numbers are due to that the predictions itself do not result in integer numbers leaving a discrepancy between the prediction and actual data which are integers 1-5.

```{r}
# Decision tree 1

target_variable <- "stars"
library(tree)

# Construct the formula using paste
formula_str <- paste(target_variable, "~ .")

# Train a decision tree model on the training data
model <- tree(as.formula(formula_str), data = trainingdata_actual)

plot(model)
text(model, pretty = 1)
title(main = "Decision Tree for Training Data")

# Predict on the test data
predictions <- predict(model, newdata = testdata_actual, type = "vector")  

# Convert both vectors to factors
predictions <- factor(predictions)
testdata_actual$stars <- factor(testdata_actual$stars)

# Create a data frame with both vectors
df <- data.frame(predictions, testdata_actual$stars)

contingency_table <- table(df)
accuracy <- sum(diag(contingency_table)) / sum(contingency_table)
cat("Accuracy:", accuracy, "\n")

print(contingency_table)

plot(model)
text(model, pretty = 1)
title(main = "Decision Tree for Test Data")

```

```{r}
# Decision Tree 2 review data based
target_variable <- "stars"
selected_columns <- c("useful", "funny", "cool")

# Train a decision tree model on the training data using specific columns
model_two <- tree(as.formula(paste(target_variable, "~", paste(selected_columns, collapse = " + "))), data = trainingdata_actual)

# Plot the decision tree
plot(model_two)
text(model_two, pretty = 1)
title(main = "Decision Tree for Training Data 2")

# Predict on the test data
predictions_two <- predict(model_two, newdata = testdata_actual, type = "vector")  

# Convert both vectors to factors
predictions_two <- factor(predictions_two)
testdata_actual$stars <- factor(testdata_actual$stars)

# Create a data frame with both vectors
df_two <- data.frame(predictions_two, testdata_actual$stars)

contingency_table_two <- table(df_two)
accuracy_two <- sum(diag(contingency_table_two)) / sum(contingency_table_two)
cat("Accuracy:", accuracy_two, "\n")

print(contingency_table_two)

plot(model_two)
text(model_two, pretty = 1)
title(main = "Decision Tree for Test Data 2")

```

```{r}
# Decision Tree 3 business and user data based
target_variable <- "stars"
selected_columns <- c("business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount", "average_stars")

# Train a decision tree model on the training data using specific columns
model_three <- tree(as.formula(paste(target_variable, "~", paste(selected_columns, collapse = " + "))), data = trainingdata_actual)

# Plot the decision tree
plot(model_three)
text(model_three, pretty = 1)
title(main = "Decision Tree for Training Data 3")

# Predict on the test data
predictions_three <- predict(model_three, newdata = testdata_actual, type = "vector")  

# Convert both vectors to factors
predictions_three <- factor(predictions_three)
testdata_actual$stars <- factor(testdata_actual$stars)

# Create a data frame with both vectors
df_three <- data.frame(predictions_three, testdata_actual$stars)

contingency_table_three <- table(df_three)
accuracy_three <- sum(diag(contingency_table_three)) / sum(contingency_table_three)
cat("Accuracy:", accuracy_three, "\n")

print(contingency_table_three)

plot(model_three)
text(model_three, pretty = 1)
title(main = "Decision Tree for Test Data 3")

```

```{r}
# Decision Tree 4 business data based
target_variable <- "stars"
selected_columns <- c("business_stars", "attributes.RestaurantsPriceRange2", "totalevaluation_attributes", "quintile_reviewcount")

# Train a decision tree model on the training data using specific columns
model_four <- tree(as.formula(paste(target_variable, "~", paste(selected_columns, collapse = " + "))), data = trainingdata_actual)

# Plot the decision tree
plot(model_four)
text(model_four, pretty = 1)
title(main = "Decision Tree for Training Data 4")

# Predict on the test data
predictions_four <- predict(model_four, newdata = testdata_actual, type = "vector")  

# Convert both vectors to factors
predictions_four <- factor(predictions_four)
testdata_actual$stars <- factor(testdata_actual$stars)

# Create a data frame with both vectors
df_four <- data.frame(predictions_four, testdata_actual$stars)

contingency_table_four <- table(df_four)
accuracy_four <- sum(diag(contingency_table_four)) / sum(contingency_table_four)
cat("Accuracy:", accuracy_four, "\n")

print(contingency_table_four)

plot(model_four)
text(model_four, pretty = 1)
title(main = "Decision Tree for Test Data 4")

```

## Concluding remarks
The assumptions were proven somewhat right in that the business and user data had an impact. What the paper aimed for as well (seeing the effect of attributes and prices of businesses as well the number of reviews before having an impact) were kind of blurred in the process. Through this analysis alone it can be concluded that the business reputation (business stars) and former behavior of the user (average stars) had the most relevance the pricing, core attributes of a restaurant and review counts, review reactions adding in slightly. It could be said for the most part the LASSO highlighted by the influence of the business data as the most effective though the decision tree also proved the influence of business reputation and former behavior of the users although to see the true accuracy we need a closer look into the predictions. If the data set had better represented data to the attributes of the business including the price might have been a more conclusive factor along side the business reputation and former behavior of users.

Some of the challenges were related with the assumption itself which required a careful process of eliminating parts that would not be used and restructuring parts so that it might be more related relevant. This part was probably the most difficult challenge to overcome because each data in the data set has its own characteristic. On top up of that splitting the independent variables into separate parts: review response, business attributes, received business reviews, and user characteristic was also a difficult process because it required a lot of assumptions from the beginning of it making the redefining process a more carefully thought out process. In the end this paper had to make some very decisive choices although a stretch of understanding the assumptions at some point had to do with the data set given to analyze.

## References
Longart, P. (2015). Consumer decision making in restaurant selection (Doctoral dissertation, Buckinghamshire New University).
Parikh, A. A., Behnke, C., Almanza, B., Nelson, D., & Vorvoreanu, M. (2017). Comparative content analysis of professional, semi-professional, and user-generated restaurant reviews. Journal of foodservice business research, 20(5), 497-511.

